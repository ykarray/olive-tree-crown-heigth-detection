{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hZaK64mEtYM",
        "outputId": "9e5ae018-3c87-4f59-fdc6-0605565e7f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "#!pip install -U torch  ultralytics\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9o2RUkpE0LP",
        "outputId": "67e1c0e4-74f0-428d-e127-c14fda502a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.123-py3-none-any.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.4/612.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.123\n",
            "--2023-06-27 07:57:00--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.70, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   239MB/s    in 13s     \n",
            "\n",
            "2023-06-27 07:57:13 (186 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "from ultralytics.yolo.data.annotator import auto_annotate\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from ultralytics.vit.sam import PromptPredictor, build_sam\n",
        "from ultralytics.yolo.utils.torch_utils import select_device\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import & downloading the sam weights\n",
        "import torch\n",
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "from PIL import Image\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAYOhSXuE52D",
        "outputId": "5801cda0-7190-4a62-a3bf-f5f1114cb8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "Torchvision version: 0.15.2+cu118\n",
            "CUDA is available: True\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-ualph1z_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-ualph1z_\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36589 sha256=f4eed6fa6bed3d48c7758a53334c30ace53170a47585b00a76ff90092ca111c2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zxx2w_hv/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "device = \"cuda\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "predictor = SamPredictor(sam)\n",
        "model=YOLO(\"/content/drive/MyDrive/olive/yolov8/runs/detect/train/weights/best.pt\")\n"
      ],
      "metadata": {
        "id": "q7Zw1YS3JzIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yolov8_detection(model, image_path):\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = model(image, stream=True)  # generator of Results objects\n",
        "\n",
        "    for result in results:\n",
        "        boxes = result.boxes  # Boxes object for bbox outputs\n",
        "        classes= boxes.cls.tolist()\n",
        "    #print(\"BOXES\",type(boxes))\n",
        "    #print(\"classes\",type(classes))\n",
        "    #print(\"classes\",len(classes))\n",
        "    index=[]\n",
        "    j=-1\n",
        "    for box in result.boxes:\n",
        "      j=j+1\n",
        "      if (box.conf[0]<0.5):\n",
        "        index.append(j)\n",
        "        print(box.conf[0])\n",
        "    bbox = boxes.xyxy.tolist()\n",
        "\n",
        "    bbox = [[int(i) for i in box] for box in bbox]\n",
        "    #print(\"BBOX\",type(bbox))\n",
        "    print(\"*************image_path\",image_path)\n",
        "    print(\"*************len(bbox)\",len(bbox))\n",
        "    print(\"*************index\",index)\n",
        "    c=-1\n",
        "    for i in index:\n",
        "      c=c+1\n",
        "      i=i+c\n",
        "      try :\n",
        "        #print(\"befor\",bbox)\n",
        "        bbox.pop(i)\n",
        "        classes.pop(i)\n",
        "        #print(\"after\",bbox)\n",
        "      except :\n",
        "        print(\"*****************************problem******************\")\n",
        "        continue\n",
        "\n",
        "    return classes,bbox, image"
      ],
      "metadata": {
        "id": "5L8V5kuJfjnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#segmentation and bounding boxes"
      ],
      "metadata": {
        "id": "PCF1d5f1Lh2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
      ],
      "metadata": {
        "id": "TyogsSzxFoq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v_Gxox6Ouz3",
        "outputId": "eef4f43d-2fd7-4592-e942-6d2968a08258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def yolov8_sam(predictor,image_path, output_path,model) :\n",
        "  predictor=predictor\n",
        "  image_path=image_path\n",
        "  classes,yolov8_boxes, image = yolov8_detection(model, image_path)\n",
        "  print(\"len(classes)\",classes)\n",
        "  print(\"len(yolov8_boxes)\",len(yolov8_boxes))\n",
        "  predictor.set_image(image)\n",
        "  input_boxes = torch.tensor(yolov8_boxes, device=predictor.device)\n",
        "  print(\"input boxes \",input_boxes)\n",
        "  print(type(input_boxes))\n",
        "  transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
        "  masks, _, _ = predictor.predict_torch(\n",
        "      point_coords=None,\n",
        "      point_labels=None,\n",
        "      boxes=transformed_boxes,\n",
        "      multimask_output=False,\n",
        "  )\n",
        "  masks=masks.to(device=\"cpu\")\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(image)\n",
        "  i=-1\n",
        "  for mask in masks:\n",
        "      print(classes[i])\n",
        "      i=i+1\n",
        "      if(int(classes[i])==1):\n",
        "        show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
        "  for box in input_boxes:\n",
        "      show_box(box.cpu().numpy(), plt.gca())\n",
        "  plt.axis('off')\n",
        "  plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "  plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZN3XaIIiFuPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolov8_sam(predictor,\"/content/drive/MyDrive/olive/data/im12.jpg\", \"/content/drive/MyDrive/olive/yolov8_SAM/image_bounding/im12.jpg\",model)"
      ],
      "metadata": {
        "id": "e5IRLW0GWKvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b81ec2-5aa7-456c-c143-3f7043321219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Tree, 60.2ms\n",
            "Speed: 18.4ms preprocess, 60.2ms inference, 87.9ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0]\n",
            "len(yolov8_boxes) 1\n",
            "input boxes  tensor([[3074, 2683, 5906, 5759]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/drive/MyDrive/olive/data/\"\n",
        "output=\"/content/drive/MyDrive/olive/yolov8_SAM/image_bounding/\"\n",
        "for filename in os.listdir(directory):\n",
        "  image_path=os.path.join(directory,filename)\n",
        "  output_path=os.path.join(output,filename)\n",
        "  yolov8_sam(predictor,image_path, output_path,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qG7dJPPKb9o",
        "outputId": "9e83d778-d9dc-4e78-9c4b-46a6ef926ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 2 Trees, 64.3ms\n",
            "Speed: 15.9ms preprocess, 64.3ms inference, 34.8ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4484, device='cuda:0')\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 639,  581, 2929, 2356],\n",
            "        [1181, 1786, 1538, 2569]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.4ms\n",
            "Speed: 4.1ms preprocess, 9.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 499,  244, 2973, 2316],\n",
            "        [1452, 1675, 1771, 2410]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 12.3ms\n",
            "Speed: 4.2ms preprocess, 12.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 504,  122, 2817, 2151],\n",
            "        [1335, 1535, 1658, 2268]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.5ms\n",
            "Speed: 5.4ms preprocess, 6.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 707,  726, 2572, 2276],\n",
            "        [1470, 1764, 1760, 2450]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.4ms\n",
            "Speed: 5.9ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 897,  466, 2831, 1841],\n",
            "        [1569, 1557, 1887, 2290]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 13.4ms\n",
            "Speed: 6.3ms preprocess, 13.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 935,  316, 2758, 1964],\n",
            "        [1674, 1442, 1967, 2148]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.6ms\n",
            "Speed: 6.4ms preprocess, 7.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 550,  251, 2807, 2133],\n",
            "        [1487, 1479, 1801, 2260]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.9ms\n",
            "Speed: 4.6ms preprocess, 9.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 454,  174, 3018, 2131],\n",
            "        [1383, 1550, 1695, 2278]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 2 Trees, 7.9ms\n",
            "Speed: 6.9ms preprocess, 7.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2675, device='cuda:0')\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 749,   96, 3001, 2155],\n",
            "        [1420, 1564, 1734, 2297]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 12.0ms\n",
            "Speed: 4.6ms preprocess, 12.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 589,  188, 2869, 2028],\n",
            "        [1571, 1479, 1891, 2228]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.9ms\n",
            "Speed: 4.1ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 529,  199, 2921, 2086],\n",
            "        [1794, 1513, 2102, 2263]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.1ms\n",
            "Speed: 4.3ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 457,  130, 2748, 1942],\n",
            "        [1317, 1432, 1664, 2173]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 10.0ms\n",
            "Speed: 4.5ms preprocess, 10.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 610,  363, 2910, 1943],\n",
            "        [1285, 1468, 1628, 2171]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 2 Trees, 7.7ms\n",
            "Speed: 4.5ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2625, device='cuda:0')\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 739,  633, 2485, 2215],\n",
            "        [1400, 1633, 1747, 2352]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 77.7ms\n",
            "Speed: 4.4ms preprocess, 77.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1109,  591, 2850, 1950],\n",
            "        [2640, 1316, 2969, 2236]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 11.5ms\n",
            "Speed: 4.1ms preprocess, 11.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1642,  467, 3301, 2003],\n",
            "        [1393, 1228, 1777, 2237]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 9.5ms\n",
            "Speed: 5.6ms preprocess, 9.5ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1697,  284, 3371, 1792],\n",
            "        [1524, 1266, 1870, 2178]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 2 Persons, 1 Tree, 17.7ms\n",
            "Speed: 19.9ms preprocess, 17.7ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2828, device='cuda:0')\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1647,  433, 3177, 1815],\n",
            "        [1454, 1287, 1847, 2353]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 2 Trees, 200.3ms\n",
            "Speed: 42.3ms preprocess, 200.3ms inference, 100.1ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(classes) [1.0, 1.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[1416,  433, 2903, 1783],\n",
            "        [   1,    0, 1096, 1024],\n",
            "        [1510, 1216, 1853, 2074]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#image segmentation with black bagroung"
      ],
      "metadata": {
        "id": "BoJkTTjshAGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, image, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "\n",
        "    # Create a black background image\n",
        "    black_image = np.zeros_like(image)\n",
        "\n",
        "    # Apply the mask to the original image\n",
        "    masked_image = np.where(mask.reshape(h, w, 1), image, black_image)\n",
        "\n",
        "    # Display the masked image\n",
        "    plt.imshow(masked_image)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "wO38ZRvReEI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yolov8_sam(predictor,image_path, output_path,model) :\n",
        "  predictor=predictor\n",
        "  image_path=image_path\n",
        "  classes,yolov8_boxes, image = yolov8_detection(model, image_path)\n",
        "  print(\"len(classes)\",classes)\n",
        "  print(\"len(yolov8_boxes)\",len(yolov8_boxes))\n",
        "  predictor.set_image(image)\n",
        "  input_boxes = torch.tensor(yolov8_boxes, device=predictor.device)\n",
        "  print(\"input boxes \",input_boxes)\n",
        "  print(type(input_boxes))\n",
        "  transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
        "  masks, _, _ = predictor.predict_torch(\n",
        "      point_coords=None,\n",
        "      point_labels=None,\n",
        "      boxes=transformed_boxes,\n",
        "      multimask_output=False,\n",
        "  )\n",
        "  masks=masks.to(device=\"cpu\")\n",
        "\n",
        "\n",
        "  i=-1\n",
        "  for mask in masks:\n",
        "      print(classes[i])\n",
        "      i=i+1\n",
        "      if(int(classes[i])==1):\n",
        "        show_mask(mask.cpu().numpy(), image, random_color=True)\n",
        "        break;\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9JNOwz9VhGpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/drive/MyDrive/olive/eq_augmented_data/part5/\"\n",
        "output=\"/content/drive/MyDrive/olive/yolov8_SAM/images_segment/\"\n",
        "for filename in os.listdir(directory):\n",
        "  image_path=os.path.join(directory,filename)\n",
        "  output_path=os.path.join(output,filename)\n",
        "  yolov8_sam(predictor,image_path, output_path,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4WJxzreiiFt",
        "outputId": "fbd769e6-9d48-4610-d21e-47c5befa5b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 3 Persons, 1 Tree, 57.5ms\n",
            "Speed: 13.9ms preprocess, 57.5ms inference, 39.2ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3222, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074354.jpg\n",
            "*************len(bbox) 4\n",
            "*************index [3]\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 395,  839, 2590, 2629],\n",
            "        [2161, 2056, 2486, 3012],\n",
            "        [2365, 2015, 2791, 3194]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 8.0ms\n",
            "Speed: 3.3ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1353, 1220, 1782, 2102],\n",
            "        [  10,  118, 1715, 1639]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 6.8ms\n",
            "Speed: 5.6ms preprocess, 6.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  29,  270, 1750, 1579],\n",
            "        [ 687, 1147,  947, 1947]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 64.8ms\n",
            "Speed: 5.0ms preprocess, 64.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075520.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 706,   67, 3496, 2150],\n",
            "        [ 653, 1445, 1082, 2776]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 19.7ms\n",
            "Speed: 4.5ms preprocess, 19.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427_1.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 511,  986, 2655, 2735],\n",
            "        [ 965, 2158, 1404, 3371],\n",
            "        [1900, 2129, 2287, 3311]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 51.4ms\n",
            "Speed: 4.6ms preprocess, 51.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 759, 1315, 3082, 7155],\n",
            "        [3054, 3052, 5411, 5145]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 8.8ms\n",
            "Speed: 3.5ms preprocess, 8.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9_2.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 364,  175, 1694, 1443],\n",
            "        [ 559, 1084,  788, 1813],\n",
            "        [ 889, 1026, 1047, 1763]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 6.1ms\n",
            "Speed: 5.9ms preprocess, 6.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  720, 2422, 2937],\n",
            "        [ 711, 2284, 1138, 3660]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 6.8ms\n",
            "Speed: 5.2ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075520_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 842,  228, 3417, 2116],\n",
            "        [ 799, 1471, 1167, 2647]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 17.5ms\n",
            "Speed: 5.2ms preprocess, 17.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  61,  224, 1869, 1584],\n",
            "        [ 933, 1152, 1195, 1982]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 3 Persons, 1 Tree, 5.9ms\n",
            "Speed: 5.9ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074354_1.jpg\n",
            "*************len(bbox) 4\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 4\n",
            "input boxes  tensor([[ 543,  846, 2725, 2672],\n",
            "        [ 346, 2040,  968, 3182],\n",
            "        [ 337, 2021,  758, 3193],\n",
            "        [ 618, 2056,  957, 3143]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.4ms\n",
            "Speed: 6.3ms preprocess, 7.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 791, 1131, 2949, 7832],\n",
            "        [3648, 2310, 6122, 5098]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.2ms\n",
            "Speed: 6.1ms preprocess, 6.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 483, 1057, 3121, 7367],\n",
            "        [3054, 2918, 5596, 5253]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 8.0ms\n",
            "Speed: 4.2ms preprocess, 8.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_082659_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 284,  240, 2956, 2546],\n",
            "        [2107, 1841, 2484, 2883]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.2ms\n",
            "Speed: 6.5ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3982,  987, 6123, 7481],\n",
            "        [ 586,  842, 4164, 5126]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 7.8ms\n",
            "Speed: 3.8ms preprocess, 7.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_082659_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   3,   35, 3026, 2582],\n",
            "        [ 546, 1809,  960, 2982]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.6ms\n",
            "Speed: 4.2ms preprocess, 9.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_5.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3979,  991, 6122, 7495],\n",
            "        [ 595,  842, 4162, 5146]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 8.8ms\n",
            "Speed: 6.4ms preprocess, 8.8ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427_2.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 637, 1078, 2546, 2610],\n",
            "        [ 921, 2121, 1252, 3177],\n",
            "        [1721, 2160, 2095, 3261]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 6.8ms\n",
            "Speed: 5.0ms preprocess, 6.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075520_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 706,   66, 3395, 2225],\n",
            "        [3081, 1476, 3491, 2788]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 13.6ms\n",
            "Speed: 3.9ms preprocess, 13.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1354, 1219, 1778, 2104],\n",
            "        [   0,  103, 1685, 1683]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 6.5ms\n",
            "Speed: 6.3ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2737, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_5.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 375,  360, 3078, 2612],\n",
            "        [ 818, 1906, 1276, 3202]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.6ms\n",
            "Speed: 5.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 503, 2959, 3053, 5287],\n",
            "        [2899, 1022, 5711, 7738]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.3ms\n",
            "Speed: 6.3ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_5.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 491, 1064, 3131, 7359],\n",
            "        [3056, 2925, 5591, 5260]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 8.6ms\n",
            "Speed: 5.1ms preprocess, 8.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080515_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 312,  620, 2922, 2535],\n",
            "        [1552, 1937, 1918, 3125]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 2 Persons, 1 Tree, 10.2ms\n",
            "Speed: 3.4ms preprocess, 10.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2578, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_081229.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1041,   62, 4071, 2259],\n",
            "        [ 845, 1636, 1279, 2805]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.7ms\n",
            "Speed: 5.7ms preprocess, 8.7ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 800, 1060, 2936, 7838],\n",
            "        [3693, 2312, 6125, 5101]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 11.1ms\n",
            "Speed: 4.7ms preprocess, 11.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 410,  767, 3129, 7786],\n",
            "        [3031, 2860, 5851, 5282]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.7ms\n",
            "Speed: 6.1ms preprocess, 7.7ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3864, 1252, 5861, 7206],\n",
            "        [ 790, 1190, 4052, 4998]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 7.7ms\n",
            "Speed: 3.8ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2684, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 376,  355, 3083, 2625],\n",
            "        [ 818, 1905, 1274, 3203]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 7.6ms\n",
            "Speed: 3.8ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 121,  273, 1858, 1571],\n",
            "        [ 932, 1172, 1185, 1944]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 6.5ms\n",
            "Speed: 5.7ms preprocess, 6.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3510, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_3.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 287,  257, 3110, 2684],\n",
            "        [ 790, 1894, 1274, 3254]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 6.2ms\n",
            "Speed: 6.3ms preprocess, 6.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9_3.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 285,   28, 1800, 1448],\n",
            "        [ 491, 1056,  778, 1911],\n",
            "        [ 882, 1015, 1083, 1881]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.7ms\n",
            "Speed: 6.0ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  921, 2141, 7721],\n",
            "        [2023,  839, 5716, 5128]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 11.8ms\n",
            "Speed: 4.7ms preprocess, 11.8ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 513, 1094, 3125, 7306],\n",
            "        [3062, 2932, 5602, 5245]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 8.8ms\n",
            "Speed: 3.6ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  25, 1220,  448, 2094],\n",
            "        [ 118,  108, 1762, 1674]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.8ms\n",
            "Speed: 4.0ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  19, 2299, 2534, 5157],\n",
            "        [3118, 1042, 5476, 7898]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 9.4ms\n",
            "Speed: 4.5ms preprocess, 9.4ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 554, 1006, 2612, 2726],\n",
            "        [ 850, 2129, 1207, 3290],\n",
            "        [1715, 2171, 2166, 3373]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 8.9ms\n",
            "Speed: 6.3ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2840, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_2.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 449,  513, 2936, 2567],\n",
            "        [ 890, 1915, 1279, 3077]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.0ms\n",
            "Speed: 4.7ms preprocess, 9.0ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 794, 1109, 2950, 7834],\n",
            "        [3640, 2324, 6121, 5095]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.9ms\n",
            "Speed: 6.3ms preprocess, 6.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 691,  907, 2935, 8085],\n",
            "        [3675, 2381, 6129, 5229]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.2ms\n",
            "Speed: 4.3ms preprocess, 9.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[4019,  628, 6144, 7791],\n",
            "        [ 488,  679, 4228, 5239]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.5ms\n",
            "Speed: 5.6ms preprocess, 6.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3976,  986, 6122, 7377],\n",
            "        [ 601,  819, 4159, 5141]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 11.4ms\n",
            "Speed: 7.1ms preprocess, 11.4ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 312,   70, 1800, 1444],\n",
            "        [ 513, 1063,  809, 1868],\n",
            "        [ 881, 1016, 1077, 1827]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 6.0ms\n",
            "Speed: 6.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080515_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  39,  455, 2921, 2627],\n",
            "        [1145, 1907, 1577, 3244]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 6.7ms\n",
            "Speed: 6.3ms preprocess, 6.7ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9_4.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 312,   68, 1799, 1441],\n",
            "        [ 513, 1064,  782, 1868],\n",
            "        [ 880, 1017, 1076, 1825]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 10.6ms\n",
            "Speed: 5.7ms preprocess, 10.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1311, 1218, 1676, 2014],\n",
            "        [ 121,  214, 1600, 1601]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 10.7ms\n",
            "Speed: 3.7ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080515.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 199,  460, 3086, 2633],\n",
            "        [1553, 1950, 1974, 3239]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.4ms\n",
            "Speed: 4.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1001, 1350, 2946, 7370],\n",
            "        [3641, 2473, 5839, 4961]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 7.1ms\n",
            "Speed: 5.5ms preprocess, 7.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3841, device='cuda:0')\n",
            "tensor(0.2743, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_1.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [1, 2]\n",
            "*****************************problem******************\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  14,  355, 2535, 2656],\n",
            "        [1844, 1899, 2299, 3203]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 16.2ms\n",
            "Speed: 4.0ms preprocess, 16.2ms inference, 10.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  656, 2470, 2950],\n",
            "        [ 658, 2302, 1108, 3710]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 7.8ms\n",
            "Speed: 3.5ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 120,  274, 1844, 1572],\n",
            "        [ 930, 1170, 1184, 1944]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 7.1ms\n",
            "Speed: 5.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427_3.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 504,  957, 2689, 2721],\n",
            "        [ 823, 2126, 1193, 3358],\n",
            "        [1725, 2177, 2187, 3462]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 10.6ms\n",
            "Speed: 3.9ms preprocess, 10.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  725, 2422, 2921],\n",
            "        [ 709, 2286, 1133, 3658]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 9.5ms\n",
            "Speed: 3.0ms preprocess, 9.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 188,  856, 2335, 2831],\n",
            "        [ 796, 2263, 1163, 3472]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.7ms\n",
            "Speed: 4.8ms preprocess, 9.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2981, 1361, 5605, 7337],\n",
            "        [ 763, 3151, 3171, 5358]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 10.6ms\n",
            "Speed: 4.5ms preprocess, 10.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_7.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2653, 1182, 4947, 6510],\n",
            "        [ 684, 2808, 2810, 4770]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.2ms\n",
            "Speed: 5.0ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3307, 1469, 6157, 8097],\n",
            "        [ 854, 3497, 3511, 5930]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.8ms\n",
            "Speed: 4.5ms preprocess, 8.8ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_flip_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2885, 1317, 5426, 7100],\n",
            "        [ 737, 3053, 3071, 5188]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.0ms\n",
            "Speed: 4.7ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_rotation_7.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 780, 1295, 3080, 7167],\n",
            "        [3046, 3075, 5395, 5096]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.4ms\n",
            "Speed: 6.9ms preprocess, 7.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_rotation_8.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 780, 1295, 3080, 7167],\n",
            "        [3046, 3075, 5395, 5096]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.2ms\n",
            "Speed: 5.6ms preprocess, 8.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_zoom_8.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 752, 1305, 3057, 7091],\n",
            "        [3031, 3025, 5365, 5105]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.4ms\n",
            "Speed: 5.1ms preprocess, 7.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_translation_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 747, 1331, 3108, 7150],\n",
            "        [3054, 3069, 5372, 5171]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.3ms\n",
            "Speed: 7.5ms preprocess, 6.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_flip_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2885, 1317, 5426, 7100],\n",
            "        [ 737, 3053, 3071, 5188]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.2ms\n",
            "Speed: 4.6ms preprocess, 9.2ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_rotation_10.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 780, 1295, 3080, 7167],\n",
            "        [3046, 3075, 5395, 5096]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.0ms\n",
            "Speed: 6.7ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_translation_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 798, 1142, 2932, 7850],\n",
            "        [3666, 2295, 6126, 5080]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.4ms\n",
            "Speed: 4.9ms preprocess, 8.4ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_rotation_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 827, 1073, 2872, 7816],\n",
            "        [3674, 2426, 6119, 5180]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.6ms\n",
            "Speed: 5.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_flip_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  22, 2277, 2524, 5158],\n",
            "        [3123, 1044, 5482, 7906]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.5ms\n",
            "Speed: 6.7ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_translation_10.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 802, 1131, 2968, 7856],\n",
            "        [3696, 2291, 6130, 5122]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.2ms\n",
            "Speed: 9.5ms preprocess, 7.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_translation_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3261, 1468, 6166, 8118],\n",
            "        [ 843, 3494, 3497, 5928]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.2ms\n",
            "Speed: 5.5ms preprocess, 8.2ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_rotation_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3302, 1496, 6178, 8084],\n",
            "        [ 847, 3477, 3504, 5889]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.5ms\n",
            "Speed: 7.9ms preprocess, 6.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_zoom_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2770, 1230, 5157, 6783],\n",
            "        [ 715, 2929, 2941, 4967]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.2ms\n",
            "Speed: 6.3ms preprocess, 7.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_rotation_7.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3283, 1547, 6199, 8078],\n",
            "        [ 856, 3419, 3415, 5797]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "1.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Annotation  for the bounding box and the segmentation"
      ],
      "metadata": {
        "id": "-Z7QgzW9wXMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yolov8_sam(predictor,image_path,model) :\n",
        "  predictor=predictor\n",
        "  image_path=image_path\n",
        "  classes,yolov8_boxes, image = yolov8_detection(model, image_path)\n",
        "  print(\"len(classes)\",classes)\n",
        "  print(\"len(yolov8_boxes)\",len(yolov8_boxes))\n",
        "  predictor.set_image(image)\n",
        "  input_boxes = torch.tensor(yolov8_boxes, device=predictor.device)\n",
        "  print(\"input boxes \",input_boxes)\n",
        "  print(type(input_boxes))\n",
        "  transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
        "  masks, _, _ = predictor.predict_torch(\n",
        "      point_coords=None,\n",
        "      point_labels=None,\n",
        "      boxes=transformed_boxes,\n",
        "      multimask_output=False,\n",
        "  )\n",
        "  #print(\"len(masks)\",len(masks))\n",
        "\n",
        "  h1=0\n",
        "  w1=0\n",
        "  h2=0\n",
        "  masks=masks.to(device=\"cpu\")\n",
        "  for i, mask in enumerate(masks):\n",
        "\n",
        "\n",
        "      binary_mask = masks[i].squeeze().numpy().astype(np.uint8)\n",
        "\n",
        "      # Find the contours of the mask\n",
        "      contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "      largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "      # Get the new bounding box\n",
        "      bbox = [int(x) for x in cv2.boundingRect(largest_contour)]\n",
        "\n",
        "      # Get the segmentation mask for object\n",
        "      segmentation = largest_contour.flatten().tolist()\n",
        "\n",
        "      # Write bounding boxes to file in YOLO format\n",
        "      for contour in contours:\n",
        "        # Get the bounding box coordinates of the contour\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        if (classes[i]==1.0):\n",
        "          if (h>h1):\n",
        "            h1=h\n",
        "            w1=w\n",
        "        if (classes[i]==0.0):\n",
        "          if (h>h2):\n",
        "            h2=h\n",
        "\n",
        "\n",
        "\n",
        "      mask=segmentation\n",
        "      # load the image\n",
        "      #width, height = image_path.size\n",
        "      img = Image.open(image_path)\n",
        "      width, height = img.size\n",
        "      # convert mask to numpy array of shape (N,2)\n",
        "      mask = np.array(mask).reshape(-1,2)\n",
        "\n",
        "      # normalize the pixel coordinates\n",
        "      mask_norm = mask / np.array([width, height])\n",
        "\n",
        "      # compute the bounding boxlen(yolov8_boxes) 2\n",
        "      xmin, ymin = mask_norm.min(axis=0)\n",
        "      xmax, ymax = mask_norm.max(axis=0)\n",
        "      bbox_norm = np.array([xmin, ymin, xmax, ymax])\n",
        "\n",
        "      # concatenate bbox and mask to obtain YOLO format\n",
        "      yolo = np.concatenate([bbox_norm, mask_norm.reshape(-1)])\n",
        "\n",
        "      # compute the bounding box\n",
        "      # write the yolo values to a text file\n",
        "      if (int(classes[i])==1):\n",
        "        with open('yolomask_format.txt', 'w') as f:\n",
        "          f.write('0 ')\n",
        "          for val in yolo:\n",
        "            f.write(\"{:.6f} \".format(val))\n",
        "          f.write('\\n')\n",
        "  try:\n",
        "    Tree_heigth=(h1/h2)*1.7\n",
        "    Tree_width=(w1/h1)*Tree_heigth\n",
        "  except:\n",
        "    print(\"************************problem*****************\")\n",
        "    Tree_heigth=0\n",
        "    Tree_width=0\n",
        "  return Tree_heigth,Tree_width\n",
        "\n"
      ],
      "metadata": {
        "id": "Pu4Mc0UIivqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/drive/MyDrive/olive/eq_augmented_data/part5/\"\n",
        "heigth=[]\n",
        "width=[]\n",
        "filename=[]\n",
        "for filen in os.listdir(directory):\n",
        "  print(filen)\n",
        "  filename.append(filen)\n",
        "  image_path=os.path.join(directory,filen)\n",
        "  h,w =yolov8_sam(predictor,image_path,model)\n",
        "  print(\"h=\",h)\n",
        "  print(\"w=\",w)\n",
        "  heigth.append(h)\n",
        "  width.append(w)"
      ],
      "metadata": {
        "id": "ejksPPwvLQkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc02c38d-b0e1-424c-f62e-151b0b9d794d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_20210603_074354.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 3 Persons, 1 Tree, 11.1ms\n",
            "Speed: 5.9ms preprocess, 11.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3222, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074354.jpg\n",
            "*************len(bbox) 4\n",
            "*************index [3]\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 395,  839, 2590, 2629],\n",
            "        [2161, 2056, 2486, 3012],\n",
            "        [2365, 2015, 2791, 3194]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 12.7ms\n",
            "Speed: 3.9ms preprocess, 12.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h= 2.693127147766323\n",
            "w= 3.0976804123711337\n",
            "im10_4.jpg\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1353, 1220, 1782, 2102],\n",
            "        [  10,  118, 1715, 1639]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.3143184421534935\n",
            "w= 3.0962199312714773\n",
            "im11_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 16.6ms\n",
            "Speed: 6.9ms preprocess, 16.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  29,  270, 1750, 1579],\n",
            "        [ 687, 1147,  947, 1947]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6494910941475824\n",
            "w= 3.6790076335877857\n",
            "IMG_20210603_075520.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 17.3ms\n",
            "Speed: 6.2ms preprocess, 17.3ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075520.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 706,   67, 3496, 2150],\n",
            "        [ 653, 1445, 1082, 2776]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.8644736842105263\n",
            "w= 3.648684210526316\n",
            "IMG_20210603_074427_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 7.8ms\n",
            "Speed: 5.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427_1.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 511,  986, 2655, 2735],\n",
            "        [ 965, 2158, 1404, 3371],\n",
            "        [1900, 2129, 2287, 3311]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6202898550724636\n",
            "w= 2.775362318840579\n",
            "im12_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.4ms\n",
            "Speed: 6.3ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 759, 1315, 3082, 7155],\n",
            "        [3054, 3052, 5411, 5145]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6396953248117668\n",
            "w= 0.6108212222027667\n",
            "im9_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 7.4ms\n",
            "Speed: 4.1ms preprocess, 7.4ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9_2.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 364,  175, 1694, 1443],\n",
            "        [ 559, 1084,  788, 1813],\n",
            "        [ 889, 1026, 1047, 1763]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.1427785419532324\n",
            "w= 3.0469050894085283\n",
            "IMG_20210603_075921_4.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 11.6ms\n",
            "Speed: 4.1ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  720, 2422, 2937],\n",
            "        [ 711, 2284, 1138, 3660]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.974060427413412\n",
            "w= 2.94900515843773\n",
            "IMG_20210603_075520_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 12.5ms\n",
            "Speed: 4.5ms preprocess, 12.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075520_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 9.4ms\n",
            "Speed: 4.5ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input boxes  tensor([[ 842,  228, 3417, 2116],\n",
            "        [ 799, 1471, 1167, 2647]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.8664660361134997\n",
            "w= 3.7508168529664663\n",
            "im11_3.jpg\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  61,  224, 1869, 1584],\n",
            "        [ 933, 1152, 1195, 1982]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6496977025392985\n",
            "w= 3.5829504232164444\n",
            "IMG_20210603_074354_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 3 Persons, 1 Tree, 7.0ms\n",
            "Speed: 5.2ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074354_1.jpg\n",
            "*************len(bbox) 4\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 4\n",
            "input boxes  tensor([[ 543,  846, 2725, 2672],\n",
            "        [ 346, 2040,  968, 3182],\n",
            "        [ 337, 2021,  758, 3193],\n",
            "        [ 618, 2056,  957, 3143]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6969045571797077\n",
            "w= 3.100343938091144\n",
            "im14_5.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.1ms\n",
            "Speed: 6.5ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 791, 1131, 2949, 7832],\n",
            "        [3648, 2310, 6122, 5098]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7566901937227812\n",
            "w= 0.6142363718276018\n",
            "im12.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 23.2ms\n",
            "Speed: 10.4ms preprocess, 23.2ms inference, 16.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 483, 1057, 3121, 7367],\n",
            "        [3054, 2918, 5596, 5253]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6955884669922798\n",
            "w= 0.6181818181818182\n",
            "IMG_20210603_082659_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 12.1ms\n",
            "Speed: 4.7ms preprocess, 12.1ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_082659_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 284,  240, 2956, 2546],\n",
            "        [2107, 1841, 2484, 2883]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.739040451552211\n",
            "w= 4.410724365004704\n",
            "im13.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 10.6ms\n",
            "Speed: 4.2ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3982,  987, 6123, 7481],\n",
            "        [ 586,  842, 4164, 5126]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 1.1847772657450077\n",
            "w= 0.8672350230414747\n",
            "IMG_20210603_082659_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 9.1ms\n",
            "Speed: 4.4ms preprocess, 9.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_082659_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   3,   35, 3026, 2582],\n",
            "        [ 546, 1809,  960, 2982]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.792419080068143\n",
            "w= 4.488926746166951\n",
            "im13_5.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.9ms\n",
            "Speed: 6.6ms preprocess, 8.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_5.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3979,  991, 6122, 7495],\n",
            "        [ 595,  842, 4162, 5146]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h= 1.1844369334767244\n",
            "w= 0.8676294361653095\n",
            "IMG_20210603_074427_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 640x480 2 Persons, 1 Tree, 7.4ms\n",
            "Speed: 4.2ms preprocess, 7.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427_2.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 637, 1078, 2546, 2610],\n",
            "        [ 921, 2121, 1252, 3177],\n",
            "        [1721, 2160, 2095, 3261]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.602924528301887\n",
            "w= 2.758490566037736\n",
            "IMG_20210603_075520_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 1 Person, 1 Tree, 8.4ms\n",
            "Speed: 3.6ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075520_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 7.4ms\n",
            "Speed: 7.1ms preprocess, 7.4ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input boxes  tensor([[ 706,   66, 3395, 2225],\n",
            "        [3081, 1476, 3491, 2788]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.862258313998453\n",
            "w= 3.733952049497293\n",
            "im10.jpg\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1354, 1219, 1778, 2104],\n",
            "        [   0,  103, 1685, 1683]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.320160366552119\n",
            "w= 3.113745704467354\n",
            "IMG_20210603_080733_5.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 11.7ms\n",
            "Speed: 4.0ms preprocess, 11.7ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2737, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_5.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 375,  360, 3078, 2612],\n",
            "        [ 818, 1906, 1276, 3202]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "************************problem*****************\n",
            "h= 0\n",
            "w= 0\n",
            "im12_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.2ms\n",
            "Speed: 6.4ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 503, 2959, 3053, 5287],\n",
            "        [2899, 1022, 5711, 7738]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.5720252167060678\n",
            "w= 0.6170370370370369\n",
            "im12_5.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.4ms\n",
            "Speed: 5.9ms preprocess, 6.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_5.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 491, 1064, 3131, 7359],\n",
            "        [3056, 2925, 5591, 5260]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6943586511188149\n",
            "w= 0.6188150015757957\n",
            "IMG_20210603_080515_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 10.2ms\n",
            "Speed: 4.1ms preprocess, 10.2ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080515_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 312,  620, 2922, 2535],\n",
            "        [1552, 1937, 1918, 3125]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.657239627434378\n",
            "w= 3.676375952582558\n",
            "IMG_20210603_081229.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 480x640 2 Persons, 1 Tree, 8.5ms\n",
            "Speed: 4.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2578, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_081229.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1041,   62, 4071, 2259],\n",
            "        [ 845, 1636, 1279, 2805]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.504774305555556\n",
            "w= 4.056684027777778\n",
            "im14_4.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.3ms\n",
            "Speed: 6.6ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 800, 1060, 2936, 7838],\n",
            "        [3693, 2312, 6125, 5101]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7567758413461538\n",
            "w= 0.6147686298076923\n",
            "im12_3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 12.8ms\n",
            "Speed: 4.6ms preprocess, 12.8ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 410,  767, 3129, 7786],\n",
            "        [3031, 2860, 5851, 5282]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7615061506150616\n",
            "w= 0.6130813081308131\n",
            "im13_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.6ms\n",
            "Speed: 5.8ms preprocess, 6.6ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3864, 1252, 5861, 7206],\n",
            "        [ 790, 1190, 4052, 4998]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 1.1945240532241557\n",
            "w= 0.8955305356533606\n",
            "IMG_20210603_080733.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 12.0ms\n",
            "Speed: 4.0ms preprocess, 12.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2684, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 376,  355, 3083, 2625],\n",
            "        [ 818, 1905, 1274, 3203]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "************************problem*****************\n",
            "h= 0\n",
            "w= 0\n",
            "im11_4.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 10.8ms\n",
            "Speed: 3.8ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 121,  273, 1858, 1571],\n",
            "        [ 932, 1172, 1185, 1944]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6365139949109415\n",
            "w= 3.6681933842239185\n",
            "IMG_20210603_080733_3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 9.3ms\n",
            "Speed: 4.1ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3510, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_3.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 287,  257, 3110, 2684],\n",
            "        [ 790, 1894, 1274, 3254]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "************************problem*****************\n",
            "h= 0\n",
            "w= 0\n",
            "im9_3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 6.6ms\n",
            "Speed: 5.6ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9_3.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 285,   28, 1800, 1448],\n",
            "        [ 491, 1056,  778, 1911],\n",
            "        [ 882, 1015, 1083, 1881]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.1459074733096086\n",
            "w= 2.9704626334519575\n",
            "im13_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.1ms\n",
            "Speed: 8.1ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  921, 2141, 7721],\n",
            "        [2023,  839, 5716, 5128]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 1.1271511985248925\n",
            "w= 0.8565304240934235\n",
            "im12_4.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 10.8ms\n",
            "Speed: 5.3ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 513, 1094, 3125, 7306],\n",
            "        [3062, 2932, 5602, 5245]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6499290556518997\n",
            "w= 0.6150874980293236\n",
            "im10_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 8.6ms\n",
            "Speed: 5.1ms preprocess, 8.6ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  25, 1220,  448, 2094],\n",
            "        [ 118,  108, 1762, 1674]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.3142201834862384\n",
            "w= 3.1309633027522934\n",
            "im14_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.2ms\n",
            "Speed: 5.5ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  19, 2299, 2534, 5157],\n",
            "        [3118, 1042, 5476, 7898]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7641452799039472\n",
            "w= 0.6169293111211166\n",
            "IMG_20210603_074427.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 7.0ms\n",
            "Speed: 5.6ms preprocess, 7.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 554, 1006, 2612, 2726],\n",
            "        [ 850, 2129, 1207, 3290],\n",
            "        [1715, 2171, 2166, 3373]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6107142857142858\n",
            "w= 2.7653911564625853\n",
            "IMG_20210603_080733_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 8.6ms\n",
            "Speed: 4.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2840, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_2.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [2]\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 449,  513, 2936, 2567],\n",
            "        [ 890, 1915, 1279, 3077]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.9639564124057\n",
            "w= 3.352975691533948\n",
            "im14.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.3ms\n",
            "Speed: 6.5ms preprocess, 7.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 794, 1109, 2950, 7834],\n",
            "        [3640, 2324, 6121, 5095]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.75731450886152\n",
            "w= 0.61432862721538\n",
            "im14_3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.6ms\n",
            "Speed: 6.8ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 691,  907, 2935, 8085],\n",
            "        [3675, 2381, 6129, 5229]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7748712814645309\n",
            "w= 0.5779319221967963\n",
            "im13_3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.0ms\n",
            "Speed: 6.3ms preprocess, 8.0ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[4019,  628, 6144, 7791],\n",
            "        [ 488,  679, 4228, 5239]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 1.1921625968708875\n",
            "w= 0.8598186869425354\n",
            "im13_4.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.6ms\n",
            "Speed: 5.5ms preprocess, 8.6ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im13_4.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3976,  986, 6122, 7377],\n",
            "        [ 601,  819, 4159, 5141]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 1.1899216469503764\n",
            "w= 0.8663235520049163\n",
            "im9.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 6.7ms\n",
            "Speed: 5.1ms preprocess, 6.7ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 312,   70, 1800, 1444],\n",
            "        [ 513, 1063,  809, 1868],\n",
            "        [ 881, 1016, 1077, 1827]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.1526119402985073\n",
            "w= 3.061691542288557\n",
            "IMG_20210603_080515_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 10.0ms\n",
            "Speed: 4.0ms preprocess, 10.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080515_1.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  39,  455, 2921, 2627],\n",
            "        [1145, 1907, 1577, 3244]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6517925247902365\n",
            "w= 3.6749046529366893\n",
            "im9_4.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 16.6ms\n",
            "Speed: 4.0ms preprocess, 16.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im9_4.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 312,   68, 1799, 1441],\n",
            "        [ 513, 1064,  782, 1868],\n",
            "        [ 880, 1017, 1076, 1825]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.130359355638166\n",
            "w= 3.031350681536555\n",
            "im10_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 9.3ms\n",
            "Speed: 4.2ms preprocess, 9.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im10_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1311, 1218, 1676, 2014],\n",
            "        [ 121,  214, 1600, 1601]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 3.3135959339263024\n",
            "w= 3.1235069885641673\n",
            "IMG_20210603_080515.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 7.4ms\n",
            "Speed: 4.4ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080515.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 199,  460, 3086, 2633],\n",
            "        [1553, 1950, 1974, 3239]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6512213740458015\n",
            "w= 3.6803053435114506\n",
            "im14_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.3ms\n",
            "Speed: 4.6ms preprocess, 9.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[1001, 1350, 2946, 7370],\n",
            "        [3641, 2473, 5839, 4961]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7806440847655598\n",
            "w= 0.6138494910729183\n",
            "IMG_20210603_080733_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 2 Trees, 9.9ms\n",
            "Speed: 4.2ms preprocess, 9.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3841, device='cuda:0')\n",
            "tensor(0.2743, device='cuda:0')\n",
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_080733_1.jpg\n",
            "*************len(bbox) 3\n",
            "*************index [1, 2]\n",
            "*****************************problem******************\n",
            "len(classes) [1.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  14,  355, 2535, 2656],\n",
            "        [1844, 1899, 2299, 3203]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "************************problem*****************\n",
            "h= 0\n",
            "w= 0\n",
            "IMG_20210603_075921_3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 8.2ms\n",
            "Speed: 4.0ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921_3.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  656, 2470, 2950],\n",
            "        [ 658, 2302, 1108, 3710]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.9842326559215135\n",
            "w= 2.868675543097407\n",
            "im11.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 10.1ms\n",
            "Speed: 6.4ms preprocess, 10.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im11.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 120,  274, 1844, 1572],\n",
            "        [ 930, 1170, 1184, 1944]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6526048284625157\n",
            "w= 3.6764930114358316\n",
            "IMG_20210603_074427_3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 2 Persons, 1 Tree, 13.5ms\n",
            "Speed: 4.1ms preprocess, 13.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_074427_3.jpg\n",
            "*************len(bbox) 3\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0, 0.0]\n",
            "len(yolov8_boxes) 3\n",
            "input boxes  tensor([[ 504,  957, 2689, 2721],\n",
            "        [ 823, 2126, 1193, 3358],\n",
            "        [1725, 2177, 2187, 3462]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.6131663974151857\n",
            "w= 2.735379644588045\n",
            "IMG_20210603_075921.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 8.7ms\n",
            "Speed: 3.7ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[   0,  725, 2422, 2921],\n",
            "        [ 709, 2286, 1133, 3658]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.9790714812085484\n",
            "w= 2.955268975681651\n",
            "IMG_20210603_075921_2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x480 1 Person, 1 Tree, 6.5ms\n",
            "Speed: 6.4ms preprocess, 6.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/IMG_20210603_075921_2.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 188,  856, 2335, 2831],\n",
            "        [ 796, 2263, 1163, 3472]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 2.9798690671031096\n",
            "w= 2.961783960720131\n",
            "im12_2_6.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 9.4ms\n",
            "Speed: 5.0ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2981, 1361, 5605, 7337],\n",
            "        [ 763, 3151, 3171, 5358]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6445989486179413\n",
            "w= 0.6114464982194335\n",
            "im12_2_7.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 19.2ms\n",
            "Speed: 7.4ms preprocess, 19.2ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_7.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2653, 1182, 4947, 6510],\n",
            "        [ 684, 2808, 2810, 4770]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.5697016067329763\n",
            "w= 0.6142501912777354\n",
            "im12_2_8.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.6ms\n",
            "Speed: 6.9ms preprocess, 6.6ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3307, 1469, 6157, 8097],\n",
            "        [ 854, 3497, 3511, 5930]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.5702366318377382\n",
            "w= 0.6138598647818071\n",
            "im12_2_flip_6.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.2ms\n",
            "Speed: 6.9ms preprocess, 7.2ms inference, 7.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_flip_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2885, 1317, 5426, 7100],\n",
            "        [ 737, 3053, 3071, 5188]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6434927307759678\n",
            "w= 0.6113329830092836\n",
            "im12_2_rotation_7.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 10.1ms\n",
            "Speed: 4.2ms preprocess, 10.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_rotation_7.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 780, 1295, 3080, 7167],\n",
            "        [3046, 3075, 5395, 5096]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.5489242609760363\n",
            "w= 0.6095854469127164\n",
            "im12_2_rotation_8.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.6ms\n",
            "Speed: 6.4ms preprocess, 6.6ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_rotation_8.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 780, 1295, 3080, 7167],\n",
            "        [3046, 3075, 5395, 5096]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.5489242609760363\n",
            "w= 0.6095854469127164\n",
            "im12_2_zoom_8.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.0ms\n",
            "Speed: 6.5ms preprocess, 7.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_zoom_8.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 752, 1305, 3057, 7091],\n",
            "        [3031, 3025, 5365, 5105]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6415150980045912\n",
            "w= 0.6111954794278651\n",
            "im12_2_translation_9.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.0ms\n",
            "Speed: 6.9ms preprocess, 7.0ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_translation_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 747, 1331, 3108, 7150],\n",
            "        [3054, 3069, 5372, 5171]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6423743652600246\n",
            "w= 0.6099282087200141\n",
            "im12_2_flip_9.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 8.6ms\n",
            "Speed: 7.0ms preprocess, 8.6ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_flip_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2885, 1317, 5426, 7100],\n",
            "        [ 737, 3053, 3071, 5188]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6434927307759678\n",
            "w= 0.6113329830092836\n",
            "im12_2_rotation_10.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.6ms\n",
            "Speed: 6.0ms preprocess, 6.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_rotation_10.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 780, 1295, 3080, 7167],\n",
            "        [3046, 3075, 5395, 5096]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.5489242609760363\n",
            "w= 0.6095854469127164\n",
            "im14_5_translation_6.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.5ms\n",
            "Speed: 7.4ms preprocess, 6.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_translation_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 798, 1142, 2932, 7850],\n",
            "        [3666, 2295, 6126, 5080]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7581943818536878\n",
            "w= 0.6136547994592157\n",
            "im14_5_rotation_9.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 13.3ms\n",
            "Speed: 5.0ms preprocess, 13.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_rotation_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 827, 1073, 2872, 7816],\n",
            "        [3674, 2426, 6119, 5180]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7709599759253686\n",
            "w= 0.6167168221486609\n",
            "im14_5_flip_9.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 10.7ms\n",
            "Speed: 4.9ms preprocess, 10.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_flip_9.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [1.0, 0.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[  22, 2277, 2524, 5158],\n",
            "        [3123, 1044, 5482, 7906]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.7660714285714285\n",
            "w= 0.6168367346938776\n",
            "im14_5_translation_10.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.8ms\n",
            "Speed: 6.9ms preprocess, 6.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im14_5_translation_10.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[ 802, 1131, 2968, 7856],\n",
            "        [3696, 2291, 6130, 5122]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.756945487310407\n",
            "w= 0.6134704910647245\n",
            "im12_2_8_translation_6.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 6.6ms\n",
            "Speed: 6.9ms preprocess, 6.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_translation_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3261, 1468, 6166, 8118],\n",
            "        [ 843, 3494, 3497, 5928]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.567189180882127\n",
            "w= 0.6144767173812817\n",
            "im12_2_8_rotation_6.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.5ms\n",
            "Speed: 5.7ms preprocess, 7.5ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_rotation_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3302, 1496, 6178, 8084],\n",
            "        [ 847, 3477, 3504, 5889]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.6284087411511234\n",
            "w= 0.61088027085257\n",
            "im12_2_8_zoom_6.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 10.9ms\n",
            "Speed: 6.5ms preprocess, 10.9ms inference, 12.8ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_zoom_6.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[2770, 1230, 5157, 6783],\n",
            "        [ 715, 2929, 2941, 4967]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.5719684461566685\n",
            "w= 0.6134470739313888\n",
            "im12_2_8_rotation_7.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x512 1 Person, 1 Tree, 7.9ms\n",
            "Speed: 5.4ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************image_path /content/drive/MyDrive/olive/eq_augmented_data/part5/im12_2_8_rotation_7.jpg\n",
            "*************len(bbox) 2\n",
            "*************index []\n",
            "len(classes) [0.0, 1.0]\n",
            "len(yolov8_boxes) 2\n",
            "input boxes  tensor([[3283, 1547, 6199, 8078],\n",
            "        [ 856, 3419, 3415, 5797]], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "h= 0.623106527093596\n",
            "w= 0.6079279556650247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create a dictionary from the lists\n",
        "data = {'filename': filename, 'heigth': heigth, 'width': width}\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/olive/eq_augmented_data/part5/yolov8_5.csv', index=False)"
      ],
      "metadata": {
        "id": "voT476-CPyhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in the directory\n",
        "csv_files = glob.glob('*.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the concatenated data\n",
        "concatenated_data = pd.DataFrame()\n",
        "\n",
        "# Iterate over each CSV file and append its data to the concatenated DataFrame\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "    concatenated_data = concatenated_data.append(df, ignore_index=True)\n",
        "\n",
        "# Save the concatenated data to a new CSV file\n",
        "concatenated_data.to_csv('concatenated.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dDR895jg7F9",
        "outputId": "0359f89f-a7f2-46f4-bb12-905d6e50b6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-0c927be3b825>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  concatenated_data = concatenated_data.append(df, ignore_index=True)\n",
            "<ipython-input-1-0c927be3b825>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  concatenated_data = concatenated_data.append(df, ignore_index=True)\n",
            "<ipython-input-1-0c927be3b825>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  concatenated_data = concatenated_data.append(df, ignore_index=True)\n",
            "<ipython-input-1-0c927be3b825>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  concatenated_data = concatenated_data.append(df, ignore_index=True)\n",
            "<ipython-input-1-0c927be3b825>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  concatenated_data = concatenated_data.append(df, ignore_index=True)\n",
            "<ipython-input-1-0c927be3b825>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  concatenated_data = concatenated_data.append(df, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NurROoRMGMRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}